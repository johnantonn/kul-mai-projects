# References for Artificial Intelligence
A non-exhaustive list of _books_, _publications_, _courses_ and _other materials_.


## üìö Books

A short list of books that covers the entire spectrum of the current AI regime.

|#|Title|Authors|Year|Link|
|:-------------|:-------------|:-------------|-------------|-------------|
|1|The Principles of Deep Learning Theory|Daniel A. Roberts, Sho Yaida, Boris Hanin|2022|[:link:](https://deeplearningtheory.com/)|
|2|Deep Learning with Python, Second Edition|Fran√ßois Chollet|2021|[:link:](https://www.manning.com/books/deep-learning-with-python-second-edition)|
|3|Artificial Intelligence: A Modern Approach, 4th Edition|Stuart Russell, Peter Norvig|2020|[:link:](https://www.pearson.com/us/higher-education/program/Russell-Artificial-Intelligence-A-Modern-Approach-4th-Edition/PGM1263338.html)|
|4|Mining of Massive Datasets|Jure Leskovec, Anand Rajaraman, Jeff Ullman|2019|[:link:](http://www.mmds.org/)|
|5|Reinforcement Learning: An Introduction|Richard Sutton, Andrew Barto|2018|[:link:](https://dl.acm.org/doi/10.5555/3312046)|
|6|Deep Learning|Ian Goodfellow, Yoshua Bengio, Aaron Courville|2016|[:link:](https://www.deeplearningbook.org/)|

---

## üìù Publications

### Opinions

Publications conveying expert opinions on the current state and future of AI/AGI.

|#|Title|Authors|Year|Link|
|:-------------|:-------------|:-------------|-------------|-------------|
|1|Why AI is Harder Than We Think|Melanie Mitchell|2021|[:link:](https://arxiv.org/abs/2104.12871)|
|2|When Combating Hype, Proceed with Caution|Samuel R. Bowman|2021|[:link:](https://arxiv.org/abs/2110.08300)|
|3|Reward is enough|David Silver et al.|2021|[:link:](https://www.sciencedirect.com/science/article/pii/S0004370221000862)|
|4|Artificial Intelligence Is Stupid and Causal Reasoning Will Not Fix It|Mark Bishop|2021|[:link:](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7874145/)|
|5|On the Measure of Intelligence|Fran√ßois Chollet|2019|[:link:](https://arxiv.org/abs/1911.01547)|
|6|Human-Level Intelligence or Animal-Like Abilities?|Adnan Darwiche|2017|[:link:](https://arxiv.org/abs/1707.04327)|
|7|Do We Need Hundreds of Classifiers to Solve Real World Classification Problems?|Manuel Fernandez-Delgado et al.|2014|[:link:](https://jmlr.org/papers/volume15/delgado14a/delgado14a.pdf)|

### Deep Learning

Publications focused on state-of-the art deep learning techniques.

|#|Title|Authors|Year|Link|
|:-------------|:-------------|:-------------|-------------|-------------|
|1|Beyond neural scaling laws: beating power law scaling via data pruning|Ben Sorscher et al.|2022|[:link:](https://arxiv.org/pdf/2206.14486v1.pdf)|
|2|Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models|Kushal Tirumala et al.|2022|[:link:](https://arxiv.org/pdf/2205.10770.pdf)|
|3|Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets|Alethea Power et al.| 2022 |[:link:](https://arxiv.org/abs/2201.02177)|
|4|Self-supervised Pretraining of Visual Features in the Wild|Priya Goyal et al.|2021|[:link:](https://arxiv.org/pdf/2103.01988.pdf?fbclid=IwAR2pqhYda6MV9r2b3Afx_0eKUiZhX-Es6Pa_FbLOqH8fglQzO2kY3yKxZE8)|
|5|Every Model Learned by Gradient Descent Is Approximately a Kernel Machine|Pedro Domingos|2020|[:link:](https://arxiv.org/abs/2012.00152)|
|6|Deep Double Descent: Where Bigger Models and More Data Hurt|Preetum Nakkiran et al.|2019|[:link:](https://arxiv.org/abs/1912.02292)|
|7|Folding: Why Good Models Sometimes Make Spurious Recommendations|Doris Xin et al.|2017|[:link:](https://dl.acm.org/doi/pdf/10.1145/3109859.3109911)|
|8|Attention is All you Need|Ashish Vaswani et al.|2017|[:link:](https://arxiv.org/abs/1706.03762)|
|9|Deep Neural Networks for YouTube Recommendations|Paul Covington et al.|2016|[:link:](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45530.pdf)|
|10|Adam: A Method for Stochastic Optimization|Diederik Kingma, Jimmy Ba|2015|[:link:](https://arxiv.org/abs/1412.6980)|

### Generative Models

Publications focused on a sub-field of deep learning related to generative models such as large language models (LLMs).

|#|Title|Authors|Year|Link|
|:-------------|:-------------|:-------------|-------------|-------------|
|1|GPT-4 Technical Report|OpenAI|2023|[:link:](https://arxiv.org/pdf/2303.08774.pdf)|
|2|Sparks of Artificial General Intelligence: Early experiments with GPT-4|S√©bastien Bubeck et al.|2023|[:link:](https://arxiv.org/abs/1911.01547)|
|3|Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models|Chenfei Wu et al.|2023|[:link:](https://arxiv.org/abs/2303.04671)|
|4|Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models|Yiheng Liu et al.|2023|[:link:](https://arxiv.org/abs/2304.01852)|
|5|GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models|Tyna Eloundou et al.|2023|[:link:](https://arxiv.org/abs/2303.10130)|
|6|High-Resolution Image Synthesis with Latent Diffusion Models|Robin Rombach et al.|2022|[:link:](https://arxiv.org/abs/2112.10752)|
|7|BLOOM: A 176B-Parameter Open-Access Multilingual Language Model|BigScience Workshop|2022|[:link:](https://arxiv.org/abs/2211.05100)|
|8|Hierarchical Text-Conditional Image Generation with CLIP Latents|Aditya Ramesh et al.|2022|[:link:](https://arxiv.org/abs/2204.06125)|
|9|Training language models to follow instructions with human feedback|OpenAI|2022|[:link:](https://arxiv.org/pdf/2203.02155.pdf)|
|10|Language Models are Few-Shot Learners|Tom B. Brown et al.|2020|[:link:](https://arxiv.org/abs/2005.14165)|
|11|Language models are unsupervised multitask learners|Alec Radford et al.|2019|[:link:](https://arxiv.org/abs/1905.04226)|
|12|BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension|Mike Lewis et al.|2019|[:link:](https://arxiv.org/abs/1910.13461)|
|13|XLNet: Generalized Autoregressive Pretraining for Language Understanding|Zhilin Yang et al.|2019|[:link:](https://arxiv.org/abs/1906.08237)|
|14|The curious case of neural text degeneration|Ari Holtzman et al.|2019|[:link:](https://arxiv.org/abs/1904.09751)|
|15|BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding|Jacob Devlin et al.|2018|[:link:](https://arxiv.org/abs/1810.04805)|
|16|Generative models of visually grounded speech signal|Karen Simonyan and Andrew Zisserman|2016|[:link:](https://arxiv.org/abs/1605.08781)|
|17|Generative modeling by estimating gradients of the data distribution|Mohammad Norouzi et al.|2016|[:link:](https://arxiv.org/abs/1511.01844)|
|18|Generative adversarial networks|Ian Goodfellow et al.|2014|[:link:](https://arxiv.org/abs/1406.2661)|
|19|Sequence to sequence learning with neural networks|Ilya Sutskever et al.|2014|[:link:](https://arxiv.org/abs/1409.3215)|
|20|Efficient Estimation of Word Representations in Vector Space|Tomas Mikolov et al.|2013|[:link:](https://arxiv.org/pdf/1301.3781.pdf)|

### Ethics & Privacy

Publications related to risks, safety, ethics and privacy of AI systems.

|#|Title|Authors|Year|Link|
|:-------------|:-------------|:-------------|-------------|-------------|
|1|Ethics guidelines for trustworthy AI|European Commission|2019|[:link:](https://op.europa.eu/en/publication-detail/-/publication/d3988569-0434-11ea-8c1f-01aa75ed71a1)|
|2|Big Data and security policies: Towards a framework for regulating the phases of analytics and use of Big Data|Dennis Broeders et al.|2017|[:link:](https://www.sciencedirect.com/science/article/pii/S0267364917300675)|
|3|Ethical, Legal and Social Challenges of Predictive Policing|Oskar Gstrein et al.|2019|[:link:](https://research.rug.nl/en/publications/ethical-legal-and-social-challenges-of-predictive-policing)|
|4|Disinformation and freedom of opinion and expression|Irene Khan|2021|[:link:](https://www.ohchr.org/EN/Issues/FreedomOpinion/Pages/Report-on-dis:link:information.aspx)|

---

## üé• Courses

|#|Title|Instructors|Institution|Year|Link|
|:-------------|:-------------|:-------------|:-------------|-------------|-------------|
|1|Introduction to Deep Learning|Alexander Amini, Ava Amini, Sadhana Lolla|MIT|2023|[:link:](http://introtodeeplearning.com/)|
|2|Deep Learning Specialization | Andrew Ng | Coursera | 2022 | [:link:](https://www.coursera.org/specializations/deep-learning?)|
|3|Introduction to Artificial Intelligence|Pieter Abbeel, Dan Klein|UC Berkeley|2018|[:link:](https://www.youtube.com/playlist?list=PLsOUugYMBBJENfZ3XAToMsg44W7LeUVhF)|
|4|Machine Learning|Andrew Ng|Stanford University|2017|[:link:](https://youtube.com/playlist?list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN)|
|5|Neural Networks for Machine Learning|Geoffrey Hinton|Coursera|2012|[:link:](https://youtube.com/playlist?list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9)|

---

## üìé Other

|#|About|Creator|Link|
|:-------------|:-------------|:-------------|-------------|
|1|AI study guides|Shervine Amidi|[:link:](https://stanford.edu/~shervine/teaching/cs-229/)|
|2|ML Visualizer|Sagnik Bhattacharya|[:link:](https://ml-visualizer.herokuapp.com/)|
|3|Outlier Detection Visualizer|Rajiv Shah|[:link:](http://projects.rajivshah.com/shiny/outlier/)|
|4|Applied Machine Learning with Python|Andreas C. M√ºller|[:link:](https://amueller.github.io/aml/index.html#)|

---

## Template table

Template for adding references.

|#|Title|Authors|Year|Link|
|:-------------|:-------------|:-------------|:-------------|-------------|
|||||[:link:]()|
